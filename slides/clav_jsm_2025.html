<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>clav: Cluster Analysis Validation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Bryer, Ph.D." />
    <meta name="date" content="2025-08-05" />
    <script src="libs/header-attrs-2.30/header-attrs.js"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/mtheme_max.css" type="text/css" />
    <link rel="stylesheet" href="assets/fonts_mtheme_max.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: center, middle, inverse, title-slide

&lt;img src='clav.png' alt='clav hex logo' align='right' width='300' /&gt;

# clav: Cluster Analysis Validation
## Joint Statistical Meeting
### Jason Bryer, Ph.D.
### August 5, 2025


---
# Overview

1. Motivation for this package.

2. Discussion of validation in the context of clustering analysis.

3. How to use the `clav` package for clustering analysis.

	a. Determine the optimal number of clusters.
	
	b. Validation the cluster solution.
	
	c. Exploring the relationship of clusters to other variables.
	
4. Shiny application.



.font70[This research was supported under grants P116F150077 and R305A210269 from the U.S.
Department of Education. However, the contents do not necessarily represent the policy of the
U.S. Department of Education, and you should not assume endorsement by the Federal
Government.]


---
# Motivating Example

The Diagnostic Assessment and Achievement of College Skills (DAACS; [www.daacs.net](https://daacs.net)) is a suite of technological and social supports designed to optimize student learning. 

Students complete assessments in self-regulated learning, writing, mathematics, and reading comprehension. They are then provided with immediate feedback in terms of one, two, and three dots (developing, emerging, and mastering, respectively) and receive customized strategies and resources based upon their results.

Prior research has shown that DAACS can improve the accuracy of predicting student success by 2% to 10% over non-DAACS models. However, those models have been **variable centric**.

In order to provide better information to help institutional staff/instructors we wish to define **profiles** using a **person centric** approach.

---
# Data Source

Data for this study was collected as part of a large scale randomized control trial.

Online institution of predominately adult learners.

Competency based program where students complete a series of competencies that when combined form course credits.

Competencies are graded on pass/fail so success is measured by students completing the equivelent of 12 credits with 6 months.


---
# Validation

Model validation is the process of estimating how well a model performs. This is often done by separating the data into two where one dataset is used to train the model and predictions are made with the second dataset.

## Supervised Methods

*Supervised models* are models where the outcome, or *truth* is known. Common supervised methods include regression, classification, and object detection.

## Unsupervised Methods

*Unsupervised models* are models where the outcome is not observed or known. Common unsupervised methods include clustering (e.g. k-means, latent profile analysis) and dimension reduction (e.g. exploratory factor analysis, principal component analysis).


---
# Clustering

Clustering is a statistical procedure that groups observations that are similar across multiple variables. Whereas principal component analysis (PCA) and exploratory factor analysis (EFA) are variable centric (i.e. columns), clustering methods are observation centric (i.e. rows).

The `clav` package is designed to work with clustering algorithms. We will use k-means clustering here (using  `stats::kmeans()`), but other methods do work.

The steps for clustering include:

1. Determine the number of clusters.

2. Validate the cluster solution.

3. Use the cluster assignments in other models.

---
# Getting started

You can download the development version from Github:


``` r
remotes::install_github('jbryer/clav')
```

Load the package and data frame:


``` r
library(clav)
data("daacs", package = "clav")
cluster_vars &lt;- c('Motivation', 'Metacognition', 'Strategies', 'Mathematics', 'Reading', 'Writing')
outcome_vars &lt;- c('FeedbackViews', 'TermSuccess')
```

We will standardize our clustering variables:


``` r
daacs &lt;- daacs |&gt; 
	dplyr::mutate(dplyr::across(dplyr::all_of(cluster_vars), clav::scale_this))
```


---
# DAACS Variables

.pull-left[
### Clustering Variables

Self-Regulated Learning measures (Likert response data ranging from 0 to 4)

* `Motivation`
* `Metacognition`
* `Strategies`

Academic measures (students complete 18 to 24 items, scores range from 0 to 1)

* `Mathematics`
* `Reading`
* `Writing`
]
.pull-right[
### Outcome Variables

* `FeedbackViews` - number of feedback pages students access within the DAACS system.
* `TermSuccess` - whether the student successfully completed 12 credits within their first term.

]

---
class: font80
# Variable Centric Approach

.pull-left[

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = FeedbackViews ~ Motivation + Metacognition + Strategies + 
#&gt;     Mathematics + Reading + Writing, data = daacs)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -21.890  -9.120  -3.327   5.361 232.315 
#&gt; 
#&gt; Coefficients:
#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)    16.8463     0.1742  96.729  &lt; 2e-16 ***
#&gt; Motivation     -1.1350     0.2113  -5.370 8.13e-08 ***
#&gt; Metacognition  -1.3912     0.2295  -6.061 1.43e-09 ***
#&gt; Strategies      1.6176     0.2430   6.657 3.03e-11 ***
#&gt; Mathematics     1.6899     0.1908   8.858  &lt; 2e-16 ***
#&gt; Reading         0.8047     0.1943   4.141 3.50e-05 ***
#&gt; Writing         2.7004     0.1848  14.614  &lt; 2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 13.91 on 6369 degrees of freedom
#&gt; Multiple R-squared:  0.07874,	Adjusted R-squared:  0.07788 
#&gt; F-statistic: 90.73 on 6 and 6369 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[

```
#&gt; 
#&gt; Call:
#&gt; glm(formula = TermSuccess ~ Motivation + Metacognition + Strategies + 
#&gt;     Mathematics + Reading + Writing, family = binomial(link = "logit"), 
#&gt;     data = daacs)
#&gt; 
#&gt; Coefficients:
#&gt;                Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)    0.795330   0.027933  28.473  &lt; 2e-16 ***
#&gt; Motivation    -0.002025   0.033495  -0.060  0.95179    
#&gt; Metacognition -0.094069   0.036811  -2.555  0.01060 *  
#&gt; Strategies     0.104083   0.038689   2.690  0.00714 ** 
#&gt; Mathematics    0.236394   0.030138   7.844 4.38e-15 ***
#&gt; Reading        0.295536   0.030575   9.666  &lt; 2e-16 ***
#&gt; Writing        0.168553   0.028544   5.905 3.53e-09 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 7981.2  on 6375  degrees of freedom
#&gt; Residual deviance: 7599.6  on 6369  degrees of freedom
#&gt; AIC: 7613.6
#&gt; 
#&gt; Number of Fisher Scoring iterations: 4
```
]

---
# Finding Optimal Clusters

Finding the optimal number of clusters is generally a balance between optimal fit statistics, parsimony, and interpretability. 

* [Davies-Bouldin Index](https://ieeexplore.ieee.org/document/4766909) (1979) - .font70[DBI is a metric used to evaluate the quality of a cluster analysis by measuring the compactness of clusters and their separation from each other. A lower DBI indicates better clustering, with well-separated and compact clusters.]
* [Calinski-Harabasz Statistic](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101) (Caliński &amp; Harabasz, 1974) - .font70[CH statistic measures the ratio of between-cluster variance to within-cluster variance, indicating how well-separated and compact the clusters are. Higher CH values generally indicate better clustering performance.]
* [Within group sum of squares](https://www.cambridge.org/core/journals/psychometrika/article/abs/who-belongs-in-the-family/5270D9B37A258A06C7CE7C0A528145F5) (Thorndike, 1953) - .font70[WSS quantifies the dispersion of data points within each cluster, with lower WSS values indicating more compact and well-defined clusters.]
* [Silhoutte score](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub) (Rousseeuw, 1986) - .font70[The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette value ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.]
* [Gap statistic](https://academic.oup.com/jrsssb/article-abstract/63/2/411/7083348?redirectedFrom=fulltext&amp;login=false) (Tibshirani, Walther, &amp; Hastie, 2001) - .font70[The Gap statistic works by comparing the within-cluster variation of the actual data to that of a null reference distribution, typically a uniform distribution. The *gap* is the difference between these two, and the optimal number of clusters is chosen where the gap statistic is maximized.]
* [Rand index](https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356) (2012) - .font70[The Rand index measures how often pairs of data points are assigned to the same or different clusters in both partitions. A higher Rand Index indicates greater similarity between the two clusterings.]


---
# Finding Optimal Clusters (cont.)

The `optimal_clusters` function will estimate the fit statistics for varying number of clusters. The default (`max_k`) is 9, but set to 6 here to reduce execution time.

The `cluster_fun` parameter defaults to `stats::kmeans`, but can other clustering functions can be used.


``` r
optimal &lt;- optimal_clusters(daacs[,cluster_vars], max_k = 6)
optimal
#&gt;   k      wss silhoutte       gap calinski_harabasz davies_bouldin rand_index
#&gt; 1 1 38250.00        NA 0.9139183               NaN            NaN         NA
#&gt; 2 2 29868.66 0.2001269 0.8825282          1788.684       1.886276  0.5002855
#&gt; 3 3 25052.96 0.2025981 0.8985740          1678.549       1.634979  0.7516727
#&gt; 4 4 22965.66 0.1524237 0.8942118          1413.590       1.766183  0.8185669
#&gt; 5 5 20747.48 0.1634359 0.9043537          1343.662       1.628235  0.7590661
#&gt; 6 6 19197.99 0.1651553 0.9158035          1264.308       1.576078  0.9055957
```

---
# Finding Optimal Clusters (cont.)


``` r
plot(optimal)
```

&lt;img src="clav_jsm_2025_files/figure-html/optimal-clusters-plot-1.png" alt="" width="100%" /&gt;

---
# Validating Clusters

For this example we are moving forward with a 5 cluster solution. The full details are available in [Cleary, Bryer, and Yu, 2025](https://github.com/daacs/Profile-Analysis).

Since there are no *known* clusters we cannot use methods typically used for supervised learning methods.

For cluster analysis, a valid cluster solution is one that is consistent.

Ullman et al (2021) proposed splitting the dataset and visually comparing the cluster solutions.

The `cluster_validation` implements this approach except that will split the dataset multiple times (default is 100). The clusters are estimated using the *training* data and cluster membership is predicted using that model with the *out-of-bag* (i.e. validation) sample.


---
# Validating Clusters


``` r
cv &lt;- cluster_validation(df = daacs[,cluster_vars], n_clusters = 5)
plot(cv)
```

&lt;img src="clav_jsm_2025_files/figure-html/cluster-validation-1.png" width="100%" /&gt;


---
# Distribution of Cluster Means


``` r
plot_distributions(cv, plot_in_sample = TRUE, plot_oob_sample = TRUE)
```

&lt;img src="clav_jsm_2025_files/figure-html/plot-distributions-1.png" alt="" width="100%" /&gt;

---
# Bootstrapping

.pull-left[
Alternatively we can using bootstrapping instead of mutually exclusive splits.

Bootstrapping is a procedure where we sample from our sample with replacement. Each bootstrap sample then has the same *n* as the original dataset.

The *out-of-bag* sample are observations that were not randomly selected for the training data.

Bootstrapping may be preferable when sample sizes are smaller.
]

.pull-right[
To use bootstrapping:

* Set `sample_size` to the number of observations.
* Set `replace = TRUE`


``` r
cv_boot &lt;- cluster_validation(
	daacs[,cluster_vars],
	n_clusters = 5,
	sample_size = nrow(daacs),
	replace = TRUE)
```
]

---
# Bootstrapping (cont.)



``` r
plot(cv_boot)
```

&lt;img src="clav_jsm_2025_files/figure-html/cluster-validation-bootstrap-plot-1.png" alt="" width="100%" /&gt;


---
# Retraining

.pull-left[
In the previous examples we predicted cluster membership from the model trained with the training data.

However, it is possible to compare the cluster solutions using two separate models: One trained with the training data and the other trained with out-of-bag (or validation) data.

]
.pull-right[
To use separate models for the two datasets, we need to define the `oob_predict_fun` parameter that implements cluster algorithm. In this example, we are wrapping the `stats::kmeans` function returning the cluster membership as a vector.


``` r
cv_retrain &lt;- cluster_validation(
	daacs[,cluster_vars],
	n_clusters = 5,
	oob_predict_fun = function(fit, newdata) {
		stats::kmeans(newdata, 5)$cluster
	}
)
```
]


---
# Retraining (cont.)


``` r
plot(cv_retrain)
```

&lt;img src="clav_jsm_2025_files/figure-html/retraining-validation-plot-1.png" alt="" width="100%" /&gt;


---
# Profile Plots


``` r
fit &lt;- stats::kmeans(daacs[,cluster_vars], centers = 5)
profile_plot(daacs[,cluster_vars], clusters = fit$cluster, 
			 df_dep = daacs[,outcome_vars], cluster_order = cluster_vars)
```

&lt;img src="clav_jsm_2025_files/figure-html/profile-plot-1.png" alt="" width="100%" /&gt;

---
# Shiny Application


``` r
clav::cluster_shiny(daacs = daacs) # NOTE: Can pass an arbitrary named parameters of data.frames
```

&lt;img src="shiny_screenshot.png" alt="" width="70%" style="display: block; margin: auto;" /&gt;

---
# Deploying Application with Your Data (`app.R`)


``` r
library(clav)                                                     # Load packages

data("daacs", package = "clav")                                   # Load some data frames
data("pisa2015", package = "clav")
pisa_usa &lt;- pisa2015 |&gt; dplyr::filter(country == "UNITED STATES")
pisa_can &lt;- pisa2015 |&gt; dplyr::filter(country == "CANADA")

data_frames &lt;- list(                                              # The data_frames list object
	"DAACS" = daacs,                                              # contains the data.frames.
	"PISA USA" = pisa_usa,                                        # The name is required and is
	"PISA Canada" = pisa_can                                      # what the user sees.
)

server &lt;- clav::clav_shiny_server                                 # Copy the server and UI Shiny
ui &lt;- clav::clav_shiny_ui                                         # functions from clav.

app_env &lt;- new.env()                                              # Create a new empty environment.
assign("data_frames", data_frames, app_env)                       # Assign data_frames to app_env.

environment(server) &lt;- as.environment(app_env)                    # Assigned the data_frames
environment(ui) &lt;- as.environment(app_env)                        # object to the Shiny functions.

shiny::shinyApp(ui = ui, server = server)                         # Run the app
```


---
class: inverse, right, middle, hide-logo, font130

.left-column[

&lt;img src="clav_jsm_2025_files/figure-html/qrcode-1.png" alt="" width="100%" height="100%" /&gt;

]

.font180[Thank you!]

[&lt;svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M440 6.5L24 246.4c-34.4 19.9-31.1 70.8 5.7 85.9L144 379.6V464c0 46.4 59.2 65.5 86.6 28.6l43.8-59.1 111.9 46.2c5.9 2.4 12.1 3.6 18.3 3.6 8.2 0 16.3-2.1 23.6-6.2 12.8-7.2 21.6-20 23.9-34.5l59.4-387.2c6.1-40.1-36.9-68.8-71.5-48.9zM192 464v-64.6l36.6 15.1L192 464zm212.6-28.7l-153.8-63.5L391 169.5c10.7-15.5-9.5-33.5-23.7-21.2L155.8 332.6 48 288 464 48l-59.4 387.3z"&gt;&lt;/path&gt;&lt;/svg&gt; jason.bryer@cuny.edu](mailto:jason.bryer@cuny.edu)  
[&lt;svg viewBox="0 0 496 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"&gt;&lt;/path&gt;&lt;/svg&gt; @jbryer](https://github.com/jbryer)  
[&lt;svg viewBox="0 0 448 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"&gt;&lt;/path&gt;&lt;/svg&gt; @jbryer@vis.social](https://vis.social/@jbryer)  
[&lt;svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"&gt;&lt;/path&gt;&lt;/svg&gt; github.com/jbryer/clav](https://github.com/jbryer/clav)   

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "solarized-light",
  "highlightLanguage": "R",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9",
  "navigation": {
    "scroll": false
  }
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<!-- Source: https://www.garrickadenbuie.com/blog/xaringan-tip-logo-all-slides/ -->
<style>
.logo {
  background-image: url(clav1.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  bottom: 2em;
  right: 0.5em;
  width: 55px;
  height: 64px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
